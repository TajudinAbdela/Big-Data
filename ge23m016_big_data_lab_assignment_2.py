# -*- coding: utf-8 -*-
"""GE23M016_Big Data Lab_Assignment_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ity9H_cCvottDXRN226dOWJO7XB4wf6l
"""

pip install apache-airflow

"""# **DataFetch Pipeline**"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
import random
import os

"""# Define the default_args dictionary to specify the start date, schedule interval, and other DAG parameters"""

default_args = {
    'owner': 'your_name',
    'start_date': datetime(2024, 1, 1),
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

"""# Define the DAG object"""

dag = DAG(
    'climate_data_pipeline',
    default_args=default_args,
    description='Pipeline for acquiring and processing climatological data',
    schedule_interval=timedelta(days=1),
)

"""# Step 1: Fetch the page containing the location-wise datasets for a specific year"""

fetch_page_task = BashOperator(
    task_id='fetch_page',
    bash_command='wget -O /path/to/save/page.html https://www.ncei.noaa.gov/data/local-climatological-data/access/{{ execution_date.year }}/',
    dag=dag,
)

"""# Step 2: Select a random subset of data files"""

def select_random_files():
    files_list = [...]  # List of available data files
    num_files_to_select = 5  # Adjust this based on your requirement
    return random.sample(files_list, num_files_to_select)

select_files_task = PythonOperator(
    task_id='select_files',
    python_callable=select_random_files,
    provide_context=True,
    dag=dag,
)

"""# Step 3: Fetch individual data files"""

def fetch_data_file(file_url):
    # Implement logic to fetch individual data files
    pass

fetch_files_tasks = []
for i in range(5):  # Assuming 5 selected files
    task = BashOperator(
        task_id=f'fetch_file_{i}',
        bash_command='wget -O /path/to/save/{{ task_instance.xcom_pull(task_ids="select_files")[%d] }} {{ task_instance.xcom_pull(task_ids="fetch_page") }}' % i,
        dag=dag,
    )
    fetch_files_tasks.append(task)

"""# Step 4: Zip the data files into an archive

"""

def zip_data_files(**kwargs):
    selected_files = kwargs['ti'].xcom_pull(task_ids='select_files')
    with ZipFile('/path/to/save/archive.zip', 'w') as zipf:
        for file in selected_files:
            zipf.write(file)

zip_files_task = PythonOperator(
    task_id='zip_files',
    python_callable=zip_data_files,
    provide_context=True,
    dag=dag,
)

"""# Step 5: Place the archive at a required location"""

move_archive_task = BashOperator(
    task_id='move_archive',
    bash_command='mv /path/to/save/archive.zip /desired/location/',
    dag=dag,
)

"""# Set up task dependencies"""

fetch_page_task >> select_files_task
select_files_task >> fetch_files_tasks
fetch_files_tasks >> zip_files_task
zip_files_task >> move_archive_task

"""# **Analytics Pipeline**"""

!apt-get install ffmpeg

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.sensors.filesystem import FileSensor
from airflow.operators.dummy_operator import DummyOperator

# Define the default_args dictionary
default_args = {
    'owner': 'your_name',
    'start_date': datetime(2024, 1, 1),
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Define the DAG object
dag = DAG(
    'climate_data_processing_pipeline',
    default_args=default_args,
    description='Pipeline for processing climatological data',
    schedule_interval='*/1 * * * *',  # Trigger every 1 minute
)

# Step 1: Wait for the archive to be available
wait_for_archive_task = FileSensor(
    task_id='wait_for_archive',
    filepath='/desired/location/archive.zip',
    poke_interval=5,
    timeout=5,
    mode='poke',
    soft_fail=True,
    dag=dag,
)

# Step 2: Check if the file is a valid archive and unzip
check_and_unzip_task = BashOperator(
    task_id='check_and_unzip',
    bash_command='unzip -o /desired/location/archive.zip -d /path/to/save/',
    dag=dag,
)

# Step 3: Extract CSV contents into a data frame and filter
def extract_and_filter_csv(**kwargs):
    # Implement Apache Beam processing logic here
    pass

extract_and_filter_task = PythonOperator(
    task_id='extract_and_filter',
    python_callable=extract_and_filter_csv,
    provide_context=True,
    dag=dag,
)

# Step 4: Compute monthly averages using Apache Beam
def compute_monthly_averages(**kwargs):
    # Implement Apache Beam monthly averages logic here
    pass

compute_monthly_averages_task = PythonOperator(
    task_id='compute_monthly_averages',
    python_callable=compute_monthly_averages,
    provide_context=True,
    dag=dag,
)

# Step 5: Create visualization using geopandas and geodatasets
def create_visualization(**kwargs):
    # Implement Apache Beam logic to create visualization and export to PNG
    pass

create_visualization_task = PythonOperator(
    task_id='create_visualization',
    python_callable=create_visualization,
    provide_context=True,
    dag=dag,
)

# Step 6: Create GIF animation using ffmpeg
create_gif_animation_task = BashOperator(
    task_id='create_gif_animation',
    bash_command='ffmpeg -i /path/to/png/files/*.png /path/to/save/animation.gif',
    dag=dag,
)

delete_csv_task = BashOperator(
    task_id='delete_csv',
    bash_command='rm /path/to/save/*.csv',
    dag=dag,)